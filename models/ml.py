from datetime import datetime
import time

import matplotlib.pyplot as plt 
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_log_error, mean_squared_error, r2_score

import xgboost as xgb
import lightgbm as lgb
import catboost as cb
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.optimizers import Adam

# í•œê¸€ì¶œë ¥
plt.rcParams['font.family'] = 'Malgun Gothic' #  Windows 'Malgun Gothic' 
plt.rcParams['axes.unicode_minus'] = False

# ë°ì´í„° ë¡œë”©
train = pd.read_csv("data/train.csv")
test = pd.read_csv("data/test.csv")
# ê²°ì¸¡ì¹˜ í™•ì¸
assert train.isnull().sum().sum() == 0
assert test.isnull().sum().sum() == 0

# ë²”ì£¼ë°ì´í„° ì²˜ë¦¬
# One-hot encoding
train = pd.concat([train.drop('Sex', axis=1), pd.get_dummies(train['Sex'], prefix='Sex')], axis=1)
test = pd.concat([test.drop('Sex', axis=1), pd.get_dummies(test['Sex'], prefix='Sex')], axis=1)
for col in ['Sex_female', 'Sex_male']:
    if col not in test.columns:
        test[col] = 0

# scaling ë³€í™˜
numeric_features = ['Age', 'Height', 'Weight', 'Duration', 'Heart_Rate', 'Body_Temp']

# íŠ¹ì„± ìŠ¤ì¼€ì¼ë§
scaler = StandardScaler()
X = pd.concat([
    pd.DataFrame(scaler.fit_transform(train[numeric_features]), columns=numeric_features),
    train[['Sex_female', 'Sex_male']]
], axis=1)

# ì •ê·œí™”ë¥¼ ìœ„í•´ ë¡œê·¸ë³€í™˜
y_log = np.log1p(train['Calories'])

# í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„í• 
# ë¬´ì‘ìœ„ ìƒ˜í”Œë§
X_train, X_val, y_train, y_val = train_test_split(
    X, y_log, test_size=0.2, random_state=42 # random_state:ì‹¤í—˜ì¬í˜„ì„±
    )

# í‰ê°€ í•¨ìˆ˜
def evaluate(name, model, X_val, y_val_log):
    # ì˜ˆì¸¡ ìˆ˜í–‰ ( ì˜ˆì¸¡ê°’ì€ ë¡œê·¸ ìŠ¤ì¼€ì¼ ìƒíƒœ)
    y_pred_log = model.predict(X_val)
    # ë¡œê·¸ ë³µì›
    y_true = np.expm1(y_val_log)
    y_pred = np.expm1(y_pred_log)

    # í‰ê°€ ì§€í‘œ ê³„ì‚°
    rmsle = np.sqrt(mean_squared_log_error(y_true, y_pred))
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)

    print(f"[{name}] RMSLE: {rmsle:.4f}, RMSE: {rmse:.2f}, R2: {r2:.2f}")
    return name, rmsle, model

# ëª¨ë¸ ë¦¬ìŠ¤íŠ¸
models = []
###################################
# 1. ë‹¤ì¤‘ íšŒê·€
lr = LinearRegression()
lr.fit(X_train, y_train)
models.append(evaluate("Linear Regression", lr, X_val, y_val))
###################################
# 2. ê²°ì • íŠ¸ë¦¬
dt = DecisionTreeRegressor(random_state=42)
dt.fit(X_train, y_train)
models.append(evaluate("Decision Tree", dt, X_val, y_val))
###################################
# 3. ê²½ì‚¬í•˜ê°•ë²•
gbr = GradientBoostingRegressor(random_state=42)
gbr.fit(X_train, y_train)
models.append(evaluate("Gradient Boosting", gbr, X_val, y_val))
###################################
# 4. LightGBM
lgb_model = lgb.LGBMRegressor(random_state=42)
lgb_model.fit(X_train, y_train)
models.append(evaluate("LightGBM", lgb_model, X_val, y_val))
###################################
# 5. XGBoost
xgb_model = xgb.XGBRegressor(random_state=42)
xgb_model.fit(X_train, y_train)
models.append(evaluate("XGBoost", xgb_model, X_val, y_val))
###################################
# 6. CatBoost
cat_model = cb.CatBoostRegressor(verbose=0, random_state=42)
cat_model.fit(X_train, y_train)
models.append(evaluate("CatBoost", cat_model, X_val, y_val))
###################################
# 7. ë”¥ëŸ¬ë‹
model_dl = Sequential([
    Input(shape=(X_train.shape[1],)),   # â† ì—¬ê¸° ì¤‘ìš”!
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),
    Dense(1)
])
model_dl.compile(optimizer=Adam(0.01), loss='mse')
# í•™ìŠµì‹œê°„ ì¸¡ì • ì‹œì‘
start_time = time.time()
# ë”¥ëŸ¬ë‹ í•™ìŠµ
model_dl.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1)
# í•™ìŠµì‹œê°„ ì¸¡ì • ë
end_time = time.time()
elapsed_time = end_time - start_time
print(f"â± ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ì‹œê°„: {elapsed_time:.2f}ì´ˆ")

y_pred_dl_log = model_dl.predict(X_val).flatten()
y_val_actual = np.expm1(y_val)
y_pred_dl = np.expm1(y_pred_dl_log)
rmsle_dl = np.sqrt(mean_squared_log_error(y_val_actual, y_pred_dl))
rmse_dl = np.sqrt(mean_squared_error(y_val_actual, y_pred_dl))
r2_dl = r2_score(y_val_actual, y_pred_dl)
print(f"[Deep Learning] RMSLE: {rmsle_dl:.4f}, RMSE: {rmse_dl:.2f}, R2: {r2_dl:.2f}")
models.append(("Deep Learning", rmsle_dl, model_dl))
###################################
# âœ… ê°€ì¥ RMSLE ë‚®ì€ ëª¨ë¸ ì„ íƒ
best_model_name, _, best_model = sorted(models, key=lambda x: x[1])[0]
print(f"\nğŸ¯ ê°€ì¥ ì¢‹ì€ ëª¨ë¸: {best_model_name}")

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡
X_test = pd.concat([
    pd.DataFrame(scaler.transform(test[numeric_features]), columns=numeric_features),
    test[['Sex_female', 'Sex_male']]
], axis=1)

# ëª¨ë¸ì´ ë”¥ëŸ¬ë‹(Sequential ëª¨ë¸)ì¼ ê²½ìš°: .predict() ê²°ê³¼ê°€ 2ì°¨ì›ì´ë¯€ë¡œ flatten í•„ìš”
if best_model_name == "Deep Learning":
    test_pred_log = best_model.predict(X_test).flatten()
else:
    test_pred_log = best_model.predict(X_test)

# ì˜ˆì¸¡ ê²°ê³¼ëŠ” log1p ìƒíƒœì´ë¯€ë¡œ expm1()ìœ¼ë¡œ ì›ë˜ ê°’(ì¹¼ë¡œë¦¬)ìœ¼ë¡œ ë³µì›
test_pred = np.expm1(test_pred_log)

# ì œì¶œ íŒŒì¼ ì €ì¥, ì–‘ì‹ì— ë§ì¶¤
submission = pd.DataFrame({
    'id': test['id'],
    'Calories': test_pred
})

# í˜„ì¬ ë‚ ì§œì™€ ì‹œê°„ì„ íŒŒì¼ëª…ì— í¬í•¨
current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
submission.to_csv(f"submissions/submission_{best_model_name}_{current_time}.csv", index=False)
print(f"ğŸ“ ì œì¶œ ì™„ë£Œ: submissions/submission_{best_model_name}_{current_time}.csv")
